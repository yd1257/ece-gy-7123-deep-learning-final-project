# -*- coding: utf-8 -*-
"""[Original Transformer]A_Confidence-score_Mechanism_For_Post-processing_OCR-ed_Tibetan_Manuscripts_v7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UFQ1PIKV5apMlcwaJYGWcadWM3hTTACs

# **ECE GY 7123 Deep Learning**

## *Final Project*:
## **A Confidence-Score Mechanism for Post-Processing OCR-ed Tibetan Manuscripts**

> Institute:


*   New York University

> Student Name:


*   Xiaoyu Wang
*   Yiren Dai

## 1. **Problem Statement**

Locating the exact words in ancient manuscripts is a crucial but time consuming task for
scholars in the humanities. In recent years, the Optical Character Recognition (OCR) technology has greatly facilitated
the digitization progress on ancient texts. However, the digitized e-text corpus contains many spelling errors. **Some handwritten Tibetan characters, such as “པ” and “བ” are indistinguishable to the naked eyes, and the OCR program cannot
capture these nuances.** Current popular spelling correction systems cannot correct Tibetan texts, and are not designed to
correct OCR-ed output.

## 2. **Project Goal**

The Google OCR output contains a confidence score for each character it recognizes. In general,
characters with higher than 80% confidence scores have an overall accuracy close to 100% based on human validation,
and characters with low confidence scores, such as 30%, require special attention. We propose to **implement a Tibetan
sequence-to-sequence model that incorporates a Confidence-score mechanism to target the OCR-ed output**, i.e., teach
the model to pay attention to the low confident character and copy the high confident characters during correction.

## 3. **Related Work**

There have been few attempts in applying these advances to the field of Tibetan studies. Powerful
deep seq2seq architectures, such as the Transformer, have demonstrated great improvement on translation tasks. In
particular, **the Attention mechanism has shown rich language understanding compared to more complex recurrent or
convolutional neural networks on English-to-German and English-to-French translation (Vaswani et al., 2017)**. Many
recent neural Grammatical Error Correction (GEC) models are trained on this type of Transformer architecture (JunczysDowmunt et al., 2018; Lichtarge et al., 2018). As a whole, our task of correcting the spelling errors in the OCR-ed Tibetan
corpus is very similar to the GEC task.

## 4. **Install Project Related Packages**

*   datasets==2.8.0
*   Levenshtein == 0.20.8
*   matplotlib == 3.2.2
*   numpy == 1.21.6
*   pyvi == 0.1.1
*   seaborn == 0.11.2
*   spacy == 3.0.9
*   torch == 1.8.1
*   torchtext == 0.9.0
*   torchvision == 0.14.0+cu116
*   transformers == 4.12.2
*   vi-core-news-lg == 0.0.1

## 5. **Import Project Related Packages**
"""

# Import the necessary modules for this project:
import copy
import io
import math
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import sacrebleu
import seaborn
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import urllib.request 
import Levenshtein as ed

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer
from torch.autograd import Variable
from torchtext.legacy import data, datasets
from tqdm import tqdm
from wordcloud import WordCloud

"""## 6. **Check Runtime's GPU Availability**

"""

train_on_gpu = torch.cuda.is_available()
# check whether CUDA is available
if not train_on_gpu:
    print('CUDA is not available')
else:
    print('CUDA is available')

"""## 7. **Introduction of the Datasets for this Project**

Buddhist Digital Resource Center (BDRC) provides both human corrected clean data and OCR-ed noisy
data for the Dege Kangyur collection (70,000 pages) on their public Github repository. We choose to use the human
corrected clean data as ground truth to train our model because it has been proofread by field experts and contains minimal
errors. The OCR-ed noisy data comes from Google OCR, and it contains a confidence score for each character.

## 8. **Download Raw Data and Preprocess Datasets**

### 8.1 **Read RAW Datasets**

*   Fetch RAW Data (part_01 and part_02) from GitHub
*   Retrive datasets to local directory
"""

# Create the local folders if NOT exist
raw_data_folder_01 = "data/raw_data/part_01/pair_data/"
raw_data_folder_02 = "data/raw_data/part_02/pair_data/"

if not os.path.isdir(raw_data_folder_01):
    os.makedirs(raw_data_folder_01)
if not os.path.isdir(raw_data_folder_02):
    os.makedirs(raw_data_folder_02)

# Downloading the csv file from Public GitHub account
raw_data_url_part_01 = "https://media.githubusercontent.com/media/yd1257/ece-gy-7123-deep-learning-final-project/main/data/raw_data/part_01/pair_data/raw_data_paired_01.csv"
raw_data_url_part_02 = "https://media.githubusercontent.com/media/yd1257/ece-gy-7123-deep-learning-final-project/main/data/raw_data/part_02/pair_data/raw_data_paired_02.csv"

raw_data_file_name_01 = "data/raw_data/part_01/pair_data/raw_data_paired_01.csv"
raw_data_file_name_02 = "data/raw_data/part_02/pair_data/raw_data_paired_02.csv"

# Save the RAW data to local folder
urllib.request.urlretrieve(raw_data_url_part_01, 
                           raw_data_file_name_01)

urllib.request.urlretrieve(raw_data_url_part_02, 
                           raw_data_file_name_02)

df_part_01 = pd.read_csv(raw_data_file_name_01)
df_part_02 = pd.read_csv(raw_data_file_name_02)

df = pd.concat([df_part_01, df_part_02])
df = df.drop(['confi_score', 'ratio'], axis=1).reset_index(drop=True)

del df_part_01
del df_part_02

df

"""### 8.2 **Read Tokenizer Datasets**

*   Fetch Tokenizer Datasets from GitHub
*   Retrive datasets to local directory


"""

# Create the local folders if NOT exist
tokenizer_folder = "data/tokenizer_data/"

if not os.path.isdir(tokenizer_folder):
    os.makedirs(tokenizer_folder)

# Downloading the files from Public GitHub account
tokenizer_train_data_url = "https://media.githubusercontent.com/media/yd1257/ece-gy-7123-deep-learning-final-project/main/data/tokenizer_data/noisy_data_train.txt"
bpe_tokenizer_url_part = "https://raw.githubusercontent.com/yd1257/ece-gy-7123-deep-learning-final-project/main/data/tokenizer_data/bpe_tokenizer.json"

tokenizer_train_data_file_name = "data/tokenizer_data/noisy_data_train.txt"
bpe_tokenizer_file_name = "data/tokenizer_data/bpe_tokenizer.json"

# Save the Tokenizer data to local folder
urllib.request.urlretrieve(tokenizer_train_data_url, 
                           tokenizer_train_data_file_name)

urllib.request.urlretrieve(bpe_tokenizer_url_part, 
                           bpe_tokenizer_file_name)

"""### 8.3 **Validate and QA the Datasets**

*   Check the size of the vocabulary
*   Validate the source and target sentence list

"""

# Initialize the tokenizer
tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = Whitespace('་')

tokenizer = tokenizer.from_file(bpe_tokenizer_file_name)

print("The size of vocabulary is: ", tokenizer.get_vocab_size())

# Set the maximum sentence length
MAX_SENT_LENGTH = 510

# Get the source and target vocabulary
src_vocab_set = list(tokenizer.get_vocab().keys())
trg_vocab_set = list(tokenizer.get_vocab().keys())

# Clean the source and target sentence lists
src_sentences_list = df.noisy_text.values.tolist()
src_sentences_list = [i[:MAX_SENT_LENGTH] for i in src_sentences_list]
trg_sentences_list = df.true_text.values.tolist()
trg_sentences_list = [i[:MAX_SENT_LENGTH] for i in trg_sentences_list]

# Split the source and target sentence lists for traning
train_src_sentences_list = src_sentences_list[:int(len(src_sentences_list) * 0.8)]
train_trg_sentences_list = trg_sentences_list[:int(len(trg_sentences_list) * 0.8)]

# Split the source and target sentence lists for test
test_src_sentences_list = src_sentences_list[int(len(src_sentences_list) * 0.8):]
test_trg_sentences_list = trg_sentences_list[int(len(trg_sentences_list) * 0.8):]

# Validate the source and target sentence list
assert len(train_src_sentences_list) == len(train_trg_sentences_list)
assert len(test_src_sentences_list) == len(test_trg_sentences_list)

"""## 9. **Clean and Process the Source and Target Sentence for Model**

### 9.1 **Clean and Process the data for modelling purpose**

*   Create the Train and Test Data
*   Create the Validation Data
*   Create the BLEU Score Data
"""

# Set the maximum sentence length, which include the start of sentence and end of sentence
MAX_SENT_LENGTH_PLUS_SOS_EOS = 512

# We only keep sentences that do not exceed (overall_limitation - 2) words, so that later when we add <s>
# and </s> to a sentence it still won't exceed the overall limitation of words.
def filter_data(src_sentences_list,
                trg_sentences_list,
                max_len):
    new_src_sentences_list, new_trg_sentences_list = [], []

    for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):
        if (max_len >= len(src_sent) > 0 and
            len(trg_sent) <= max_len and 
            len(trg_sent)) > 0:
            
            # Add the source sentence to the new source sentence list
            new_src_sentences_list.append(src_sent)

            # Add the target sentence to the new source sentence list
            new_trg_sentences_list.append(trg_sent)

    return new_src_sentences_list, new_trg_sentences_list

# Filter the training data
train_src_sentences_list, train_trg_sentences_list = filter_data(
    train_src_sentences_list, 
    train_trg_sentences_list, 
    max_len=MAX_SENT_LENGTH)

# Filter the test data
test_src_sentences_list, test_trg_sentences_list = filter_data(
    test_src_sentences_list, 
    test_trg_sentences_list, 
    max_len=MAX_SENT_LENGTH)

# We take 10% of the training data to be the validation set.
num_val = int(len(train_src_sentences_list) * 0.1)
val_src_sentences_list = train_src_sentences_list[:num_val]
val_trg_sentences_list = train_trg_sentences_list[:num_val]

# Exclude the validation sentences from training
train_src_sentences_list = train_src_sentences_list[num_val:]
train_trg_sentences_list = train_trg_sentences_list[num_val:]

# take 5000 sentences to compute BLEU score
BLEU_src_sentences_list = val_src_sentences_list[:500]
BLEU_trg_sentences_list = val_trg_sentences_list[:500]

"""### 9.2 **Show the Processed Data with Basic Statistic Results**

*   Show the basic model data information
*   Show sample noisy data
"""

# Show the processed data with basic statistic results:
print("Number of training (src, trg) sentence pairs: %d" %
      len(train_src_sentences_list))

print("Number of validation (src, trg) sentence pairs: %d" %
      len(val_src_sentences_list))

print("Number of testing (src, trg) sentence pairs: %d" %
      len(test_src_sentences_list))

src_vocab_set = ['<pad>'] + src_vocab_set
trg_vocab_set = ['<pad>'] + trg_vocab_set
print("Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d" %
      len(src_vocab_set))
print("Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d" %
      len(trg_vocab_set))

length = [len(sent) for sent in train_src_sentences_list]
print('Training sentence avg. length: %d ' % np.mean(length))
print('Training sentence length at 95-percentile: %d' %
      np.percentile(length, 95))
print('Training sentence length distribution '
      '(x-axis is length range and y-axis is count):\n')

plt.hist(length, bins=5)
plt.show()

print('Example noisy input: ' + str(train_src_sentences_list[0]))
print('Its target clean output: ' + str(train_trg_sentences_list[0]))

"""### 9.3 **Build Vocabulary Data**

*   tokenize the data

"""

# Create the tokenize function for different languages
def tokenize_vi(text):
    return tokenizer.encode(text).tokens

def tokenize_en(text):
    return tokenizer.encode(text).tokens

# Generate the source and target data with field
SRC = data.Field(tokenize=tokenize_vi, 
                 pad_token='<pad>')

TRG = data.Field(tokenize=tokenize_en, 
                 pad_token='<pad>',
                 init_token='<s>', 
                 eos_token='</s>')

fields = [("src", SRC),("trg", TRG)]

# Create the example function
def get_examples(src_sent_list, 
                 trg_sent_list):
    
    examples = []
    for src_sent, trg_sent in zip(src_sent_list, trg_sent_list):
        example = data.Example.fromlist([src_sent, trg_sent], fields=fields)
        examples.append(example)

    return examples

# Get the train_examples
train_examples = get_examples(train_src_sentences_list,
                              train_trg_sentences_list)

# Get the val_examples
val_examples = get_examples(val_src_sentences_list,
                            val_trg_sentences_list)

# Get the test_examples
test_examples = get_examples(test_src_sentences_list,
                             test_trg_sentences_list)

# Get the BLEU_examples
BLEU_examples = get_examples(BLEU_src_sentences_list,
                             BLEU_trg_sentences_list)

train = data.Dataset(train_examples, 
                     fields)
val = data.Dataset(val_examples, 
                   fields)
test = data.Dataset(test_examples, 
                    fields)
BLEU_test = data.Dataset(BLEU_examples, 
                         fields)

SRC.build_vocab(train.src)
TRG.build_vocab(train.trg)

"""## 10. **Create the Customized Model Classes**"""

# Create the Batch Data Class
class Batch:
    """
    Object for holding a batch of data with mask during training.
    """
    def __init__(self, 
                 src, 
                 trg=None, 
                 pad=0):
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum()
    
    @staticmethod
    def make_std_mask(tgt, 
                      pad):
        """
        Create a mask to hide padding and future words.
        """
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(
            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        
        return tgt_mask

# Create the MyIterator Class from data.Iterator
class MyIterator(data.Iterator):
    def create_batches(self):
        if self.train:
            def pool(d, 
                     random_shuffler):
                for p in data.batch(d, 
                                    self.batch_size * 100):
                    p_batch = data.batch(
                        sorted(p, 
                               key=self.sort_key),
                        self.batch_size, 
                        self.batch_size_fn)
                    for b in random_shuffler(list(p_batch)):
                        yield b
            self.batches = pool(self.data(), 
                                self.random_shuffler)
            
        else:
            self.batches = []
            for b in data.batch(self.data(), 
                                self.batch_size,
                                self.batch_size_fn):
                self.batches.append(sorted(b, 
                                           key=self.sort_key))

def rebatch(pad_idx, 
            batch):
    """
    Fix order in torchtext to match ours
    """
    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)
    return Batch(src, 
                 trg, 
                 pad_idx)

# Commented out IPython magic to ensure Python compatibility.
# Set the context for Seaborn
seaborn.set_context(context="talk")
# %matplotlib inline

"""## 11. **Detailed Model Implementation**
Our project will have two phases: data preparation and language modeling. First, we will
feature-engineer our raw data into a structured dataframe as shown in Table 1. We need to align the OCR-ed noisy data to
the ground truth data. They are not naturally aligned but both contain a set of IDs. Then, we will modify a sequence-tosequence model with an encoder and decoder, similar to a translation model. The encoder takes in the OCR-ed noisy
sentence and the decoder outputs the corrected sentence, then we will compare the corrections to the ground truth sentence.
Here we want to introduce an attention model named Confidence-score. The Attention mechanism solved the problem of
long-term dependency between paired sentences, and the Self-Attention captured mutual dependencies between words of
a sentence. We expect our Confidence-score mechanism can capture the distributions and intra-dependencies of the low
confident characters to improve the model’s performance. 
"""

# Create the EncoderDecoder Class based on nn.Module
class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. Base for this and many 
    other models.
    """
    def __init__(self, 
                 encoder, 
                 decoder, 
                 src_embed, 
                 tgt_embed, 
                 generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
        
    def forward(self, 
                src, 
                tgt, 
                src_mask, 
                tgt_mask):
        "Take in and process masked src and target sequences."
        return self.decode(self.encode(src, src_mask), 
                           src_mask,
                           tgt, 
                           tgt_mask)
    
    def encode(self, 
               src, 
               src_mask):
        return self.encoder(self.src_embed(src), 
                            src_mask)
    
    def decode(self, 
               memory, 
               src_mask, 
               tgt, 
               tgt_mask):
        return self.decoder(self.tgt_embed(tgt), 
                            memory, 
                            src_mask, 
                            tgt_mask)

# Create the Generator Class based on nn.Module
class Generator(nn.Module):
    "Define standard linear + softmax generation step."
    def __init__(self, 
                 d_model, 
                 vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, 
                              vocab)

    def forward(self, 
                x):
        return F.log_softmax(self.proj(x), 
                             dim=-1)

# Produce N identical layers
def clones(module, 
           N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

# Create the Encoder Class based on nn.Module
class Encoder(nn.Module):
    """
    Core encoder is a stack of N layers
    """
    def __init__(self, 
                 layer, 
                 N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, 
                             N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, 
                x, 
                mask):
        """
        Pass the input (and mask) through each layer in turn.
        """
        for layer in self.layers:
            x = layer(x, 
                      mask)

        return self.norm(x)

# Create the EncoderLayer Class based on nn.Module
class EncoderLayer(nn.Module):
    """
    Encoder is made up of self-attn and feed forward (defined below)
    """
    def __init__(self, 
                 size, 
                 self_attn, 
                 feed_forward, 
                 dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, 
                x, 
                mask):
        """
        Follow Figure 1 (left) for connections.
        """
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)

# Create the Decoder Class based on nn.Module
class Decoder(nn.Module):
    """
    Generic N layer decoder with masking.
    """
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, 
                             N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, 
                x,
                memory, 
                src_mask, 
                tgt_mask):
        for layer in self.layers:
            x = layer(x, 
                      memory, 
                      src_mask, 
                      tgt_mask)
        return self.norm(x)

# Create the DecoderLayer Class based on nn.Module
class DecoderLayer(nn.Module):
    """
    Decoder is made of self-attn, src-attn, and feed forward (defined below)
    """
    def __init__(self, 
                 size, 
                 self_attn, 
                 src_attn, 
                 feed_forward, 
                 dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
        
    def forward(self, 
                x, 
                memory, 
                src_mask, 
                tgt_mask):
        """
        Follow Figure 1 (right) for connections.
        """
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))

        return self.sublayer[2](x, self.feed_forward)

# Create the LayerNorm Class based on nn.Module
class LayerNorm(nn.Module):
    """
    Construct a layernorm module (See citation for details).
    """
    def __init__(self, 
                 features, 
                 eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, 
                      keepdim=True)
        std = x.std(-1, 
                    keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

# Create the SublayerConnection Class based on nn.Module
class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, 
                 size, 
                 dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, 
                x, 
                sublayer):
        """
        Apply residual connection to any sublayer with the same size.
        """
        return x + self.dropout(sublayer(self.norm(x)))

# Create the subsequent_mask function
def subsequent_mask(size):
    """
    Mask out subsequent positions.
    """
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(subsequent_mask) == 0

# Create the attention function
def attention(query, 
              key, 
              value, 
              mask=None, 
              dropout=None):
    """
    Compute 'Scaled Dot Product Attention'
    """
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, 
                       dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

# Create the MultiHeadedAttention Class based on nn.Module
class MultiHeadedAttention(nn.Module):
    def __init__(self, 
                 h, 
                 d_model, 
                 dropout=0.1):
        """
        Take in model size and number of heads.
        """
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, 
                query, 
                key, 
                value, 
                mask=None):
        """
        Implements Figure 2
        """
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = attention(query, 
                                 key,
                                 value, 
                                 mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)

# Create the PositionwiseFeedForward Class based on nn.Module
class PositionwiseFeedForward(nn.Module):
    """
    Implements FFN equation.
    """
    def __init__(self, 
                 d_model, 
                 d_ff, 
                 dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, 
                             d_ff)
        self.w_2 = nn.Linear(d_ff, 
                             d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

# Create the Embeddings Class based on nn.Module
class Embeddings(nn.Module):
    def __init__(self, 
                 d_model, 
                 vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, 
                                d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)

# Create the PositionalEncoding Class based on nn.Module
class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, 
                 d_model, 
                 dropout, 
                 max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, 
                         d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)

"""## 12. **Build Functions for Training and Evaluation**"""

# Construct the model
def make_model(src_vocab, 
               tgt_vocab, 
               N=6, 
               d_model=512, 
               d_ff=2048, 
               h=8, 
               dropout=0.1):
    """
    Helper: Construct a model from hyperparameters.
    """
    c = copy.deepcopy

    attn = MultiHeadedAttention(h, 
                                d_model)
    
    ff = PositionwiseFeedForward(d_model, 
                                 d_ff, dropout)
    
    position = PositionalEncoding(d_model, 
                                  dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, 
                             c(attn), 
                             c(ff), 
                             dropout), 
                N),
        Decoder(DecoderLayer(d_model, 
                             c(attn), 
                             c(attn), 
                             c(ff), 
                             dropout), 
                N),
        nn.Sequential(Embeddings(d_model, src_vocab),
                      c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), 
                      c(position)),
        Generator(d_model, tgt_vocab))
    
    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model

# Create the function for run_epoch
def run_epoch(data_iter, 
              model, 
              loss_compute):
    """
    Standard Training and Logging Function
    """
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, batch in tqdm(list(enumerate(data_iter))):
        out = model.forward(batch.src, 
                            batch.trg, 
                            batch.src_mask, 
                            batch.trg_mask)
        
        loss = loss_compute(out, 
                            batch.trg_y, 
                            batch.ntokens)
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                    (i, loss / batch.ntokens, tokens / elapsed))
            
            start = time.time()
            tokens = 0
    return total_loss / total_tokens

# Set the global variable for max_src_in_batch, max_tgt_in_batch
global max_src_in_batch, max_tgt_in_batch

def batch_size_fn(new, 
                  count, 
                  sofar):
    """
    Keep augmenting batch and calculate total number of tokens + padding.
    """
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch,  
                           len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch,  
                           len(new.trg) + 2)
    src_elements = count * max_src_in_batch
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)

# Create the NoamOpt Class
class NoamOpt:
    """
    Optim wrapper that implements rate.
    """
    def __init__(self, 
                 model_size, 
                 factor, 
                 warmup, 
                 optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0
        
    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()
        
    def rate(self, 
             step = None):
        """
        Implement `lrate` above
        """
        if step is None:
            step = self._step
        return self.factor * \
            (self.model_size ** (-0.5) *
            min(step ** (-0.5), step * self.warmup ** (-1.5)))
        
def get_std_opt(model):
    return NoamOpt(model.src_embed[0].d_model, 
                   2, 
                   4000,
                   torch.optim.Adam(model.parameters(), 
                                    lr=0, 
                                    betas=(0.9, 0.98), 
                                    eps=1e-9))

# Create the LabelSmoothing Class based on nn.Module
class LabelSmoothing(nn.Module):
    """
    Implement label smoothing.
    """
    def __init__(self, 
                 size, 
                 padding_idx, 
                 smoothing=0.0):
        
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None
        
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        
        return self.criterion(x, Variable(true_dist, requires_grad=False))

# Create the SimpleLossCompute Class
class SimpleLossCompute:
    "A simple loss compute and train function."
    def __init__(self, 
                 generator, 
                 criterion, 
                 opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
        
    def __call__(self, 
                 x, 
                 y, 
                 norm):
        x = self.generator(x)
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), 
                              y.contiguous().view(-1)) / norm
        if self.opt is not None:
            loss.backward()
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return loss.item() * norm

"""## 13. **Execute the Model Training Process**"""

pad_idx = 0
model = make_model(len(SRC.vocab), 
                   len(TRG.vocab), 
                   N=3)
device = 'cuda'
model.to(device)

criterion = LabelSmoothing(size=len(TRG.vocab), 
                           padding_idx=pad_idx, 
                           smoothing=0.1)
criterion.to(device)

BATCH_SIZE = 2048

train_iter = MyIterator(train, 
                        batch_size=BATCH_SIZE, 
                        device=device,
                        repeat=False, 
                        sort_key=lambda x: (len(x.src), len(x.trg)),
                        batch_size_fn=batch_size_fn, 
                        train=True)

valid_iter = MyIterator(val, 
                        batch_size=BATCH_SIZE, 
                        device=device,
                        repeat=False, 
                        sort_key=lambda x: (len(x.src), len(x.trg)),
                        batch_size_fn=batch_size_fn, 
                        train=False)

num_epochs = 10
model_opt = NoamOpt(model.src_embed[0].d_model, 
                    1, 
                    2000,
                    torch.optim.Adam(model.parameters(), 
                                     lr=0, 
                                     betas=(0.9, 0.98), 
                                     eps=1e-9))

df_score = pd.DataFrame(columns=['epoch', 'score'])
for epoch in range(num_epochs):
    model.train()
    run_epoch((rebatch(pad_idx, b) for b in train_iter), 
              model,
              SimpleLossCompute(model.generator, 
                                criterion, 
                                opt=model_opt))
    model.eval()
    with torch.no_grad():
        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter),
                         model, 
                         SimpleLossCompute(model.generator, criterion, opt=None))
        
    print("Epoch {} validation loss: ".format(epoch), loss.item())

    df_score = df_score.append({'epoch': epoch, 'score': loss.item()}, ignore_index=True)
    df_score.to_csv('df_score', index=False)

tokenizer.get_vocab_size()

"""Tokenizer using aug_train_data:

tokenizer_vocab = 100
Epoch 4 validation loss:  0.17219483852386475

Tokenizer using noisy_train_data:

tokenizer_vocab = 100
Epoch 4 validation loss:  0.17959578335285187

tokenizer_vocab = 200
Epoch 4 validation loss:  0.17511048913002014

tokenizer_vocab = 300
Epoch 4 validation loss:  0.17061378061771393

Epoch 9 validation loss:  0.15088631212711334

tokenizer_vocab = 500

Epoch 9 validation loss:  0.12872740626335144


**ratio: 0.72**

tokenizer_vocab = 500

Epoch 9 validation loss:  0.13648764789104462

**ratio: 0.75**

tokenizer_vocab = 500

Epoch 9 validation loss:  0.14383184909820557

Accuracy: 0.9167
"""

# Create the local folders if NOT exist
best_model_data_folder = "best_model/"

if not os.path.isdir(best_model_data_folder):
    os.makedirs(best_model_data_folder)

# save model
best_model_data_file_name = "best_model/token500_v1.pth"

torch.save(model.state_dict(), best_model_data_file_name)
#model.load_state_dict(torch.load(best_model_data_file_name))

def greedy_decode(model, 
                  src, 
                  src_mask, 
                  max_len, 
                  start_symbol):
    memory = model.encode(src, 
                          src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    prob_list = []
    for i in range(max_len-1):
        out = model.decode(memory, src_mask, 
                           Variable(ys), 
                           Variable(subsequent_mask(ys.size(1))
                                    .type_as(src.data)))
        logit = model.generator(out[:, -1])
        prob = torch.nn.functional.softmax(logit, dim=1)
        p, next_word = torch.max(prob, dim = 1)
        prob_list.append(p.cpu().detach().numpy()[0])
        next_word = next_word.item()
        if next_word == TRG.vocab.stoi['</s>']:
            break
        ys = torch.cat([ys, 
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys, prob_list

model.eval()
sent = 'དང༔གནདཔ་དང་།དྲལ་བ་དང༌།དབུག་པ་དང་།གནས་ལཔོའདི་ལྟསྟེ།'

src = torch.LongTensor([[SRC.vocab.stoi[w] for w in sent]])
src = Variable(src).to(device)
src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2).to(device)
out, _ = greedy_decode(model, src, src_mask, 
                    max_len=60, start_symbol=TRG.vocab.stoi["<s>"])
print("Translation:", end="\t")
trans = "<s> "
for i in range(1, out.size(1)):
    sym = TRG.vocab.itos[out[0, i]]
    if sym == "</s>": break
    trans += sym + " "
print(trans)

"""## 14. **Model Visualization**

https://www.kaggle.com/fulrose/how-to-apply-new-font-to-matplotlib-easily
"""

tgt_sent = trans.split()

def draw(output, x, y, ax):
    seaborn.heatmap(output.cpu().data.numpy(), 
                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, 
                    cbar=False, ax=ax)
    
for layer in range(3):
    fig, axs = plt.subplots(1,4, figsize=(20, 10))
    print("Encoder Layer", layer+1)
    for h in range(4):
        draw(model.encoder.layers[layer].self_attn.attn[0, h].data, 
            sent, sent if h ==0 else [], ax=axs[h])
    plt.show()
    
for layer in range(3):
    fig, axs = plt.subplots(1,4, figsize=(20, 10))
    print("Decoder Self Layer", layer+1)
    for h in range(4):
        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(tgt_sent)], 
            tgt_sent, tgt_sent if h ==0 else [], ax=axs[h])
    plt.show()
    print("Decoder Src Layer", layer+1)
    fig, axs = plt.subplots(1,4, figsize=(20, 10))
    for h in range(4):
        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(sent)], 
            sent, tgt_sent if h ==0 else [], ax=axs[h])
    plt.show()

tgt_sent = trans.split()

def draw(output, x, y, ax):
    seaborn.heatmap(output.cpu().data.numpy(), 
                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, 
                    cbar=False, ax=ax)
    
fig, axs = plt.subplots(1,3, figsize=(20, 10))
for layer in range(3):
    print("Encoder Layer", layer)
    att_enc = torch.mean(model.encoder.layers[layer].self_attn.attn[0, :].data, axis=0)
    draw(att_enc, sent, sent, ax=axs[layer])
plt.show()
    
fig, axs = plt.subplots(1,3, figsize=(20, 10))
for layer in range(3):
    print("Decoder Self Layer", layer)
    att_dec_self = torch.mean(model.decoder.layers[layer].self_attn.attn[0, :].data[:len(tgt_sent), :len(tgt_sent)], axis=0)
    draw(att_dec_self, tgt_sent, tgt_sent if layer ==0 else [], ax=axs[layer])
plt.show()

fig, axs = plt.subplots(1,3, figsize=(20, 10))
for layer in range(3):
    print("Decoder Src Layer", layer+1)
    att_dec_src = torch.mean(model.decoder.layers[layer].self_attn.attn[0, :].data[:len(tgt_sent), :len(sent)], axis=0)
    draw(att_dec_src, sent, tgt_sent if layer ==0 else [], ax=axs[layer])
plt.show()

"""## 15. **Visualizing Correction Result**"""

tt_src_sentences_list = ['དང༔གནདཔ་དང་།དྲལ་བ་དང༌།དབུག་པ་དང་།གནས་ལཔོའདི་ལྟསྟེ།']
tt_trg_sentences_list = ['དང་། གཅད་པ་དང་། དྲལ་བ་དང་། དབུག་པ་དང་། གནས་ལྔ་པོ་འདི་ལྟ་སྟེ།']

tt_examples = get_examples(tt_src_sentences_list,
                            tt_trg_sentences_list)

tt = data.Dataset(tt_examples, fields)

tt_iter = MyIterator(tt, batch_size=BATCH_SIZE, device=device,
                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                        batch_size_fn=batch_size_fn, train=False)

for i, batch in enumerate(tt_iter):
    
    src = batch.src.transpose(0, 1)[:1]
    # print(src)
    src_mask = (src != SRC.vocab.stoi["<pad>"]).unsqueeze(-2)

    # print(confi)
    out, prob_list = greedy_decode(model, src, src_mask, 
                        max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS,
                        start_symbol=TRG.vocab.stoi["<s>"])
    print('prob: ', prob_list)
    print('Original Noisy Sentence: ', tt_src_sentences_list[i])

    print("Model Corrected Sentence: ", end="")
    pred = ""
    for i in range(1, out.size(1)):
        sym = TRG.vocab.itos[out[0, i]]
        if sym == "</s>": break
        print(sym, end ="")
        pred += sym
        
    print()
    print("Ground Truth Sentence: ", end="\t")
    for i in range(1, batch.trg.size(0)):
        sym = TRG.vocab.itos[batch.trg.data[i, 0]]
        if sym == "</s>": break
        print(sym, end ="")
    print()
    if i == 1:
        break

"""## 16. **Compute Accuracy**"""

def get_diff_token(a, b):
  mb = ed.matching_blocks(ed.editops(a,b), a, b)
  match_idx = [(t[0], t[0]+t[2]) for t in mb]
  match_idx = [np.arange(t[0], t[1]) for t in match_idx]
  match_idx = np.concatenate(match_idx, axis=0 )
  all_index = np.arange(len(a))
  not_same = np.array(list(set(all_index).difference(match_idx)))
  # group cluster
  if len(not_same) == 0:
    res = []
  elif len(not_same) == 1:
    res = [[not_same[0]]]
  else:
    res = []
    cur_group = [not_same[0]]
    for i in range(1, len(not_same)):
      if cur_group[-1] + 1 == not_same[i]:
        cur_group.append(not_same[i])
        if i == len(not_same) - 1:
          res.append(cur_group)
      else:
        res.append(cur_group)
        cur_group = [not_same[i]]
  diff_token = ["".join(a[i] for i in grp) for grp in res]

  return diff_token

diff_A_list, diff_B_list, diff_C_list = [], [], []

for j in range(200):
  print(j)
  sent_data = df.reset_index().loc[j]
  sent = sent_data.noisy_text
  sent_true = sent_data.true_text
  print('Original Noisy Sentence: ', sent)

  src = torch.LongTensor([[SRC.vocab.stoi[w] for w in sent]])
  src = Variable(src).to(device)
  src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2).to(device)
  out, prob_list = greedy_decode(model, src, src_mask,
                      max_len=512, start_symbol=TRG.vocab.stoi["<s>"])
  print("Translation:", end="\t")
  pred = ""
  for i in range(1, out.size(1)):
      sym = TRG.vocab.itos[out[0, i]]
      if sym == "</s>": break
      pred += sym
  print(pred)
  print("Ground Truth Sentence: ", sent_true)

  # compare noisy with true, a = noisy, b = true
  a, b = sent, sent_true
  diff_A = get_diff_token(a, b)
  diff_A_list.append(diff_A)
  # compare pred with true, a = pred, b = true
  a, b = pred, sent_true
  diff_B = get_diff_token(a, b)
  diff_B_list.append(diff_B)
  # compare noisy with pred, a = noisy, b = pred
  a, b = sent, pred
  diff_C = get_diff_token(a, b)
  diff_C_list.append(diff_C)

# flatten list of lists
def flatten(A):
    rt = []
    for i in A:
        if isinstance(i,list): rt.extend(flatten(i))
        else: rt.append(i)
    return rt

diff_A_list = flatten(diff_A_list)
diff_B_list = flatten(diff_B_list)
diff_C_list = flatten(diff_C_list)

A_tokens = [tokenizer.encode(a).tokens for a in diff_A_list]
A_tokens = flatten(A_tokens)
B_tokens = [tokenizer.encode(b).tokens for b in diff_B_list]
B_tokens = flatten(B_tokens)
C_tokens = [tokenizer.encode(c).tokens for c in diff_C_list]
C_tokens = flatten(C_tokens)

def get_counter(tokens):
  counter = {}
  for token in tokens:
    if token not in counter:
      counter[token] = 0
    counter[token] += 1
  return counter

A = get_counter(A_tokens)
B = get_counter(B_tokens)
C = get_counter(C_tokens)

wordcloud = WordCloud(background_color="white", 
                      font_path='font/himalaya.ttf',
                      max_words=1000, contour_width=3, 
                      contour_color='steelblue')
A_plot = wordcloud.generate_from_frequencies(frequencies=A)
plt.figure()
plt.imshow(A_plot, interpolation="bilinear")
plt.axis("off")
plt.show()

B_plot = wordcloud.generate_from_frequencies(frequencies=B)
plt.figure()
plt.imshow(B_plot, interpolation="bilinear")
plt.axis("off")
plt.show()

C_plot = wordcloud.generate_from_frequencies(frequencies=C)
plt.figure()
plt.imshow(C_plot, interpolation="bilinear")
plt.axis("off")
plt.show()

s_pred_list = []
s_org_list = []
for k in range(100):
  sent_data = df.reset_index().loc[k]
  sent = sent_data.noisy_text
  sent_true = sent_data.true_text
  sent_true = sent_true.replace(' ', '')
  print('Original Noisy Sentence: ', sent)

  src = torch.LongTensor([[SRC.vocab.stoi[w] for w in sent]])
  src = Variable(src).to(device)
  src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2).to(device)
  out, prob_list = greedy_decode(model, src, src_mask, 
                      max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS, start_symbol=TRG.vocab.stoi["<s>"])
  print("Translation:", end="\t")
  pred = ""
  for i in range(1, out.size(1)):
      sym = TRG.vocab.itos[out[0, i]]
      if sym == "</s>": break
      pred += sym
  print(pred)
  print("Ground Truth Sentence: ", sent_true)

  mb_pred = ed.matching_blocks(ed.editops(pred, sent_true), pred, sent_true)
  mb_org = ed.matching_blocks(ed.editops(sent, sent_true), sent, sent_true)
  matched_org = ''.join([sent[x[0]:x[0]+x[2]] for x in mb_org])
  org_len = max([len(sent_true), len(sent)])
  s_org = len(matched_org)/org_len
  s_org_list.append(s_org)
  #print(f'matched original token in true token: \n{matched_org}')
  print(f'score before correction: {s_org:.4f}')

  matched_pred = ''.join([pred[x[0]:x[0]+x[2]] for x in mb_pred])
  s_pred = len(matched_pred)/len(sent_true)
  s_pred_list.append(s_pred)
  #print(f'matched pred token in true token: \n{matched_pred}')
  print(f'score after correction: {s_pred:.4}')

print(f'score before correction: {np.mean(s_org_list):.4}')
print(f'score after correction: {np.mean(s_pred_list):.4}')

np.mean(s_pred_list), np.mean(s_org_list)

mb = ed.matching_blocks(ed.editops(tt_trg_sentences_list[0], pred), tt_trg_sentences_list[0], pred)
idx = range(len(tt_trg_sentences_list[0]))
correct_idx = [idx[x[0]:x[0]+x[2]] for x in mb]
correct_score = np.zeros(len(tt_trg_sentences_list[0]))
for r in correct_idx:
  for num in r:
    correct_score[num] = 1
acc = np.mean(correct_score)
print(f'Accuracy: {acc:.4f}')

true_tokens = tokenizer.encode(tt_trg_sentences_list[0]).tokens

pred_tokens = tokenizer.encode(pred).tokens
pred_attn = [(k, v) for k, v in zip(pred_tokens, prob_list)]

# to display in ipython notebook
from IPython.display import display, HTML
def colorize(words, color_array, color_code):
    # words is a list of words
    # color_array is an array of numbers between 0 and 1 of length equal to words
    cmap = matplotlib.cm.get_cmap(color_code)
    template = '<span class="barcode"; style="color: black; background-color: {}">{}</span>'
    colored_string = ''
    for word, color in zip(words, color_array):
        color = matplotlib.colors.rgb2hex(cmap(color)[:3])
        colored_string += template.format(color, '&nbsp' + word + '&nbsp')
    return colored_string

print(f'\nNOISY TEXT: {tt_src_sentences_list}')
# color_confi_list = tt_confi_list[0]
# noisy_s = colorize(noisy_tokens, color_confi_list, 'twilight')
# display(HTML(noisy_s))

print('\nMODEL CORRECTED TEXT: ')
def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    return np.exp(x) / np.sum(np.exp(x), axis=0)

color_prob_list = 1- np.array(prob_list)
# color_prob_list = softmax(color_prob_list)
pred_s = colorize(pred_tokens, color_prob_list, 'OrRd')
display(HTML(pred_s))

print('\nGROUND TRUTH TEXT: ')
color_correct_score = 1- np.array(correct_score)
pred_s = colorize(tt_trg_sentences_list[0], color_correct_score, 'OrRd')
display(HTML(pred_s))

"""## 18.Compute BLEU Score"""

def lookup_words(x, vocab):
    return [vocab.itos[i] for i in x]

def compute_BLEU(model, data_iter):
    bleu_score = []

    model.eval()
    for batch in tqdm(data_iter):
        src = batch.src.transpose(0, 1)[:1]
        trg = batch.trg
        src_mask = (src != SRC.vocab.stoi["<pad>"]).unsqueeze(-2)
        result, _ = greedy_decode(model, src, src_mask,
                               max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS,
                               start_symbol=TRG.vocab.stoi["<s>"])

        # remove <s>
        trg = trg[1:, 0]
        # remove </s> and <pad>
        trg = trg[:torch.where(trg == TRG.vocab.stoi["</s>"])[0][0]]

        pred = " ".join(lookup_words(result.squeeze(0)[1:], vocab=TRG.vocab))
        targ = " ".join(lookup_words(trg, vocab=TRG.vocab))
        print("Pred: ", pred)
        print("Target: ", targ)
        print()
        s = sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score
        print(f'score: {s}')
        bleu_score.append(s)

    return bleu_score

BLEU_test_iter = MyIterator(BLEU_test, batch_size=1, device=device,
                       repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                       batch_size_fn=None, train=False)

print("BLEU score: ", np.mean(compute_BLEU(model, BLEU_test_iter)))